{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7a2512-361e-4c9e-84e3-6151ed58505a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "# Albumentations for advanced augmentation\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Display versions\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54007b30-c0b9-4e2f-bf24-e73f32ef32eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \n",
    "    DATA_PATH = '/kaggle/input/kvasirseg/Kvasir-SEG'\n",
    "    IMAGES_PATH = os.path.join(DATA_PATH, 'images')\n",
    "    MASKS_PATH = os.path.join(DATA_PATH, 'masks')\n",
    "    OUTPUT_PATH = '/kaggle/working'\n",
    "    \n",
    "    \n",
    "    IMG_SIZE = 352              # Input image size \n",
    "    BATCH_SIZE = 20             # Batch size for training\n",
    "    EPOCHS = 100                # Number of training epochs\n",
    "\n",
    "    BASE_LR = 0.0001           # Initial learning rate\n",
    "    LR_POWER = 0.9             # Polynomial decay power\n",
    "        \n",
    "    ATROUS_RATES = [1, 6, 12, 18]  # Atrous convolution rates\n",
    "    ASPP_FILTERS = 256              # Number of filters in ASPP\n",
    "    \n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    NUM_WORKERS = 2\n",
    "    PIN_MEMORY = True\n",
    "    \n",
    "    TRAIN_RATIO = 0.7\n",
    "    VAL_RATIO = 0.15\n",
    "    TEST_RATIO = 0.15\n",
    "    \n",
    "    RANDOM_SEED = 42\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(Config.OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "print(\"âœ“ Configuration loaded successfully\")\n",
    "print(f\"  Device: {Config.DEVICE}\")\n",
    "print(f\"  Image Size: {Config.IMG_SIZE}x{Config.IMG_SIZE}\")\n",
    "print(f\"  Batch Size: {Config.BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {Config.EPOCHS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada36b62-7e30-469a-8300-3bc396983f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolypDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for Polyp Segmentation.\n",
    "    \n",
    "    Loads images and corresponding binary masks for polyp detection.\n",
    "    Applies augmentation transforms during training.\n",
    "    \n",
    "    Args:\n",
    "        image_paths (list): List of paths to input images\n",
    "        mask_paths (list): List of paths to segmentation masks\n",
    "        transform: Albumentations transform pipeline\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, image_paths, mask_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Verify all files exist\n",
    "        for img_path, mask_path in zip(image_paths[:5], mask_paths[:5]):\n",
    "            assert os.path.exists(img_path), f\"Image not found: {img_path}\"\n",
    "            assert os.path.exists(mask_path), f\"Mask not found: {mask_path}\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image and convert to RGB\n",
    "        image = cv2.imread(self.image_paths[idx])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Load mask and binarize\n",
    "        mask = cv2.imread(self.mask_paths[idx], cv2.IMREAD_GRAYSCALE)\n",
    "        mask = (mask > 127).astype(np.uint8)  # Binary threshold\n",
    "        \n",
    "        # Apply augmentation pipeline\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "        \n",
    "        # Add channel dimension to mask\n",
    "        mask = mask.unsqueeze(0).float()\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "print(\"âœ“ PolypDataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3457b23-662e-4235-a7cb-f60946f42511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transforms():\n",
    "    \"\"\"\n",
    "    Training augmentation pipeline with aggressive augmentation.\n",
    "    Includes geometric and color transformations to improve generalization.\n",
    "    \"\"\"\n",
    "    return A.Compose([\n",
    "        A.Resize(Config.IMG_SIZE, Config.IMG_SIZE),\n",
    "        \n",
    "        # Geometric augmentations\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.3),\n",
    "        A.Rotate(limit=15, p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "        \n",
    "        # Color augmentations\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "        A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.3),\n",
    "        \n",
    "        # Noise\n",
    "        A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
    "        \n",
    "        # Normalization (ImageNet statistics)\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "\n",
    "def get_val_transforms():\n",
    "    \"\"\"\n",
    "    Validation/Test augmentation pipeline.\n",
    "    Only resizing and normalization - no data augmentation.\n",
    "    \"\"\"\n",
    "    return A.Compose([\n",
    "        A.Resize(Config.IMG_SIZE, Config.IMG_SIZE),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "print(\"âœ“ Augmentation pipelines defined\")\n",
    "print(\"  Train: Geometric + Color augmentations + Noise\")\n",
    "print(\"  Val/Test: Resize + Normalization only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4c5aa9-30bd-431b-973b-fd28044e5e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASPP(nn.Module):\n",
    "    \"\"\"\n",
    "    Atrous Spatial Pyramid Pooling module.\n",
    "    \n",
    "    Captures multi-scale contextual information using parallel atrous convolutions\n",
    "    with different dilation rates, plus global pooling.\n",
    "    \n",
    "    Args:\n",
    "        in_channels (int): Number of input channels\n",
    "        out_channels (int): Number of output channels\n",
    "        atrous_rates (list): List of dilation rates for atrous convolutions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, atrous_rates):\n",
    "        super(ASPP, self).__init__()\n",
    "        \n",
    "        # 1x1 convolution branch\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Atrous convolution branches (3x3 with different dilation rates)\n",
    "        self.atrous_convs = nn.ModuleList()\n",
    "        for rate in atrous_rates:\n",
    "            self.atrous_convs.append(nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 3, \n",
    "                         padding=rate, dilation=rate, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ))\n",
    "        \n",
    "        # Global average pooling branch\n",
    "        self.global_pool = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Projection layer to combine all branches\n",
    "        total_channels = out_channels * (len(atrous_rates) + 2)\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv2d(total_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        size = x.shape[2:]\n",
    "        \n",
    "        # Apply all parallel branches\n",
    "        features = [self.conv1(x)]\n",
    "        \n",
    "        for atrous_conv in self.atrous_convs:\n",
    "            features.append(atrous_conv(x))\n",
    "        \n",
    "        # Global pooling branch (upsampled to match spatial dimensions)\n",
    "        global_feat = self.global_pool(x)\n",
    "        global_feat = F.interpolate(global_feat, size=size, \n",
    "                                    mode='bilinear', align_corners=False)\n",
    "        features.append(global_feat)\n",
    "        \n",
    "        # Concatenate all features and project\n",
    "        features = torch.cat(features, dim=1)\n",
    "        output = self.project(features)\n",
    "        \n",
    "        return output\n",
    "\n",
    "print(\"âœ“ ASPP module defined\")\n",
    "print(f\"  Atrous rates: {Config.ATROUS_RATES}\")\n",
    "print(f\"  Output channels: {Config.ASPP_FILTERS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ed00d5-b040-460e-bf41-96fabdd0bb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLabV3Plus(nn.Module):\n",
    "    \"\"\"\n",
    "    DeepLabV3+ architecture for semantic segmentation.\n",
    "    \n",
    "    Architecture components:\n",
    "    - Encoder: ResNet50 backbone with atrous convolutions\n",
    "    - ASPP: Multi-scale feature extraction\n",
    "    - Decoder: Combines high-level and low-level features\n",
    "    - Classifier: Final segmentation head\n",
    "    \n",
    "    Args:\n",
    "        num_classes (int): Number of output classes (1 for binary segmentation)\n",
    "        backbone (str): Backbone network (default: 'resnet50')\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=1, backbone='resnet50'):\n",
    "        super(DeepLabV3Plus, self).__init__()\n",
    "        \n",
    "        # ========== Backbone (ResNet50) ==========\n",
    "        resnet = resnet50(pretrained=True)\n",
    "        \n",
    "        # Encoder stages\n",
    "        self.layer0 = nn.Sequential(\n",
    "            resnet.conv1,\n",
    "            resnet.bn1,\n",
    "            resnet.relu,\n",
    "            resnet.maxpool\n",
    "        )\n",
    "        self.layer1 = resnet.layer1  # Output: 256 channels (low-level features)\n",
    "        self.layer2 = resnet.layer2  # Output: 512 channels\n",
    "        self.layer3 = resnet.layer3  # Output: 1024 channels\n",
    "        self.layer4 = resnet.layer4  # Output: 2048 channels (high-level features)\n",
    "        \n",
    "        # ========== ASPP Module ==========\n",
    "        self.aspp = ASPP(2048, Config.ASPP_FILTERS, Config.ATROUS_RATES)\n",
    "        \n",
    "        # ========== Low-level Feature Projection ==========\n",
    "        # Reduce channels from 256 to 48 for efficient concatenation\n",
    "        self.low_level_conv = nn.Sequential(\n",
    "            nn.Conv2d(256, 48, 1, bias=False),\n",
    "            nn.BatchNorm2d(48),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # ========== Decoder ==========\n",
    "        # Combines ASPP output (256 channels) and low-level features (48 channels)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(Config.ASPP_FILTERS + 48, 256, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Conv2d(256, 256, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        \n",
    "        # ========== Segmentation Head ==========\n",
    "        self.classifier = nn.Conv2d(256, num_classes, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        input_size = x.shape[2:]\n",
    "        \n",
    "        # ========== Encoder ==========\n",
    "        x = self.layer0(x)\n",
    "        low_level_feat = self.layer1(x)  # Save for decoder skip connection\n",
    "        x = self.layer2(low_level_feat)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        # ========== ASPP ==========\n",
    "        x = self.aspp(x)\n",
    "        \n",
    "        # Upsample to match low-level feature size\n",
    "        x = F.interpolate(x, size=low_level_feat.shape[2:], \n",
    "                         mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # ========== Process Low-level Features ==========\n",
    "        low_level_feat = self.low_level_conv(low_level_feat)\n",
    "        \n",
    "        # ========== Decoder ==========\n",
    "        # Concatenate high-level and low-level features\n",
    "        x = torch.cat([x, low_level_feat], dim=1)\n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        # ========== Classification ==========\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        # Upsample to original input size\n",
    "        x = F.interpolate(x, size=input_size, \n",
    "                         mode='bilinear', align_corners=False)\n",
    "        \n",
    "        return x\n",
    "\n",
    "print(\"âœ“ DeepLabV3+ model defined\")\n",
    "print(\"  Backbone: ResNet50 (pretrained)\")\n",
    "print(\"  ASPP with atrous rates:\", Config.ATROUS_RATES)\n",
    "print(\"  Decoder: Skip connection with low-level features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba94cfb-5a33-4952-8d4e-13c768858b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Dice Loss for segmentation tasks.\n",
    "    \n",
    "    Measures overlap between prediction and ground truth.\n",
    "    Effective for handling class imbalance in medical imaging.\n",
    "    \n",
    "    Args:\n",
    "        smooth (float): Smoothing factor to avoid division by zero\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, smooth=1.0):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        pred = torch.sigmoid(pred)\n",
    "        pred = pred.view(-1)\n",
    "        target = target.view(-1)\n",
    "        \n",
    "        intersection = (pred * target).sum()\n",
    "        dice_coeff = (2. * intersection + self.smooth) / \\\n",
    "                     (pred.sum() + target.sum() + self.smooth)\n",
    "        \n",
    "        return 1 - dice_coeff\n",
    "\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Combined BCE and Dice Loss.\n",
    "    \n",
    "    Combines Binary Cross-Entropy (for pixel-wise accuracy) and\n",
    "    Dice Loss (for shape and boundary accuracy).\n",
    "    \n",
    "    Args:\n",
    "        alpha (float): Weight for BCE loss (1-alpha for Dice loss)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0.5):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.dice = DiceLoss()\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        bce_loss = self.bce(pred, target)\n",
    "        dice_loss = self.dice(pred, target)\n",
    "        return self.alpha * bce_loss + (1 - self.alpha) * dice_loss\n",
    "\n",
    "print(\"âœ“ Loss functions defined\")\n",
    "print(\"  Combined Loss = Î± * BCE + (1-Î±) * Dice\")\n",
    "print(f\"  Î± = 0.5 (equal weighting)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe62079c-4daf-4c3e-aae3-b2d6eea5c24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(pred, target, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Calculate Intersection over Union (IoU / Jaccard Index).\n",
    "    \n",
    "    Args:\n",
    "        pred: Model predictions (logits)\n",
    "        target: Ground truth masks\n",
    "        threshold: Threshold for binarization\n",
    "    \n",
    "    Returns:\n",
    "        IoU score (float)\n",
    "    \"\"\"\n",
    "    pred = (torch.sigmoid(pred) > threshold).float()\n",
    "    intersection = (pred * target).sum()\n",
    "    union = pred.sum() + target.sum() - intersection\n",
    "    iou = (intersection + 1e-7) / (union + 1e-7)\n",
    "    return iou.item()\n",
    "\n",
    "\n",
    "def calculate_dice(pred, target, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Calculate Dice Coefficient (F1 Score for segmentation).\n",
    "    \n",
    "    Args:\n",
    "        pred: Model predictions (logits)\n",
    "        target: Ground truth masks\n",
    "        threshold: Threshold for binarization\n",
    "    \n",
    "    Returns:\n",
    "        Dice score (float)\n",
    "    \"\"\"\n",
    "    pred = (torch.sigmoid(pred) > threshold).float()\n",
    "    intersection = (pred * target).sum()\n",
    "    dice = (2. * intersection + 1e-7) / (pred.sum() + target.sum() + 1e-7)\n",
    "    return dice.item()\n",
    "\n",
    "print(\"âœ“ Evaluation metrics defined\")\n",
    "print(\"  - IoU (Intersection over Union)\")\n",
    "print(\"  - Dice Coefficient\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e442a16-8ee8-4f50-abdc-4ce4702f5b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolynomialLR:\n",
    "    \"\"\"\n",
    "    Polynomial learning rate decay scheduler.\n",
    "    \n",
    "    Implements: lr = base_lr * (1 - iter/max_iter)^power\n",
    "    \n",
    "    As described in the DeepLabV3+ paper for stable training.\n",
    "    \n",
    "    Args:\n",
    "        optimizer: PyTorch optimizer\n",
    "        max_iterations: Total number of training iterations\n",
    "        power: Polynomial power (default: 0.9)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, optimizer, max_iterations, power=0.9):\n",
    "        self.optimizer = optimizer\n",
    "        self.max_iterations = max_iterations\n",
    "        self.power = power\n",
    "        self.base_lr = Config.BASE_LR\n",
    "        self.current_iter = 0\n",
    "    \n",
    "    def step(self):\n",
    "        \"\"\"Update learning rate and increment iteration counter.\"\"\"\n",
    "        lr = self.base_lr * (1 - self.current_iter / self.max_iterations) ** self.power\n",
    "        \n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        \n",
    "        self.current_iter += 1\n",
    "        return lr\n",
    "    \n",
    "    def get_lr(self):\n",
    "        \"\"\"Get current learning rate.\"\"\"\n",
    "        return self.optimizer.param_groups[0]['lr']\n",
    "\n",
    "print(\"âœ“ Polynomial LR scheduler defined\")\n",
    "print(f\"  Base LR: {Config.BASE_LR}\")\n",
    "print(f\"  Power: {Config.LR_POWER}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8bd4ce-e4e5-45b3-a4c9-bd27358dc449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, scheduler, device):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        loader: Training DataLoader\n",
    "        criterion: Loss function\n",
    "        optimizer: PyTorch optimizer\n",
    "        scheduler: Learning rate scheduler\n",
    "        device: Device to run on (CPU/GPU)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (avg_loss, avg_iou, avg_dice)\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_iou = 0\n",
    "    total_dice = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training', leave=False)\n",
    "    \n",
    "    for images, masks in pbar:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update learning rate\n",
    "        current_lr = scheduler.step()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        batch_iou = calculate_iou(outputs, masks)\n",
    "        batch_dice = calculate_dice(outputs, masks)\n",
    "        \n",
    "        # Accumulate metrics\n",
    "        total_loss += loss.item()\n",
    "        total_iou += batch_iou\n",
    "        total_dice += batch_dice\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'iou': f'{batch_iou:.4f}',\n",
    "            'dice': f'{batch_dice:.4f}',\n",
    "            'lr': f'{current_lr:.6f}'\n",
    "        })\n",
    "    \n",
    "    # Calculate epoch averages\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    avg_iou = total_iou / len(loader)\n",
    "    avg_dice = total_dice / len(loader)\n",
    "    \n",
    "    return avg_loss, avg_iou, avg_dice\n",
    "\n",
    "print(\"âœ“ Training function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7fe637-2e9a-4068-9903-54c843f19dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Validate the model.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        loader: Validation DataLoader\n",
    "        criterion: Loss function\n",
    "        device: Device to run on (CPU/GPU)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (avg_loss, avg_iou, avg_dice)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_iou = 0\n",
    "    total_dice = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc='Validation', leave=False)\n",
    "        \n",
    "        for images, masks in pbar:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            batch_iou = calculate_iou(outputs, masks)\n",
    "            batch_dice = calculate_dice(outputs, masks)\n",
    "            \n",
    "            # Accumulate metrics\n",
    "            total_loss += loss.item()\n",
    "            total_iou += batch_iou\n",
    "            total_dice += batch_dice\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'iou': f'{batch_iou:.4f}',\n",
    "                'dice': f'{batch_dice:.4f}'\n",
    "            })\n",
    "    \n",
    "    # Calculate averages\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    avg_iou = total_iou / len(loader)\n",
    "    avg_dice = total_dice / len(loader)\n",
    "    \n",
    "    return avg_loss, avg_iou, avg_dice\n",
    "\n",
    "print(\"âœ“ Validation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84af1304-b7c9-4389-b5d3-54aea2a2b1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, save_path):\n",
    "    \"\"\"\n",
    "    Plot training history with loss, IoU, and Dice metrics.\n",
    "    \n",
    "    Args:\n",
    "        history (dict): Dictionary containing training metrics\n",
    "        save_path (str): Path to save the plot\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Loss plot\n",
    "    axes[0, 0].plot(history['train_loss'], label='Train Loss', \n",
    "                   linewidth=2, marker='o', markersize=3)\n",
    "    axes[0, 0].plot(history['val_loss'], label='Val Loss', \n",
    "                   linewidth=2, marker='s', markersize=3)\n",
    "    axes[0, 0].set_title('Loss over Epochs', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0, 0].set_ylabel('Loss', fontsize=12)\n",
    "    axes[0, 0].legend(fontsize=11)\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # IoU plot\n",
    "    axes[0, 1].plot(history['train_iou'], label='Train IoU', \n",
    "                   linewidth=2, marker='o', markersize=3)\n",
    "    axes[0, 1].plot(history['val_iou'], label='Val IoU', \n",
    "                   linewidth=2, marker='s', markersize=3)\n",
    "    axes[0, 1].set_title('IoU over Epochs', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0, 1].set_ylabel('IoU', fontsize=12)\n",
    "    axes[0, 1].legend(fontsize=11)\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Dice plot\n",
    "    axes[1, 0].plot(history['train_dice'], label='Train Dice', \n",
    "                   linewidth=2, marker='o', markersize=3)\n",
    "    axes[1, 0].plot(history['val_dice'], label='Val Dice', \n",
    "                   linewidth=2, marker='s', markersize=3)\n",
    "    axes[1, 0].set_title('Dice Coefficient over Epochs', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1, 0].set_ylabel('Dice', fontsize=12)\n",
    "    axes[1, 0].legend(fontsize=11)\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Summary table\n",
    "    axes[1, 1].axis('off')\n",
    "    best_epoch = np.argmax(history['val_iou'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14858b1f-e86e-4477-8359-5ae7074b2407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, loader, device, num_samples=5, save_path=None):\n",
    "    \"\"\"\n",
    "    Visualize model predictions alongside ground truth.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained PyTorch model\n",
    "        loader: DataLoader for test/validation set\n",
    "        device: Device to run on\n",
    "        num_samples: Number of samples to visualize\n",
    "        save_path: Path to save the figure\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    images_list, masks_list, preds_list = [], [], []\n",
    "    \n",
    "    # Collect predictions\n",
    "    with torch.no_grad():\n",
    "        for images, masks in loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = torch.sigmoid(outputs) > 0.5\n",
    "            \n",
    "            images_list.append(images.cpu())\n",
    "            masks_list.append(masks.cpu())\n",
    "            preds_list.append(preds.cpu())\n",
    "            \n",
    "            if len(images_list) * images.size(0) >= num_samples:\n",
    "                break\n",
    "    \n",
    "    # Concatenate and select samples\n",
    "    images_list = torch.cat(images_list)[:num_samples]\n",
    "    masks_list = torch.cat(masks_list)[:num_samples]\n",
    "    preds_list = torch.cat(preds_list)[:num_samples]\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(12, 4 * num_samples))\n",
    "    \n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Denormalize image\n",
    "        img = images_list[i].permute(1, 2, 0).numpy()\n",
    "        img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "        img = np.clip(img, 0, 1)\n",
    "        \n",
    "        # Plot original image\n",
    "        axes[i, 0].imshow(img)\n",
    "        axes[i, 0].set_title('Original Image', fontsize=12, fontweight='bold')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # Plot ground truth\n",
    "        axes[i, 1].imshow(masks_list[i].squeeze(), cmap='gray')\n",
    "        axes[i, 1].set_title('Ground Truth', fontsize=12, fontweight='bold')\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        # Plot prediction\n",
    "        axes[i, 2].imshow(preds_list[i].squeeze(), cmap='gray')\n",
    "        axes[i, 2].set_title('Prediction', fontsize=12, fontweight='bold')\n",
    "        axes[i, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"âœ“ Predictions saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "print(\"âœ“ Visualization functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed5647e-efe3-4ce9-b1cc-188c37eb0496",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"DATASET PREPARATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(Config.RANDOM_SEED)\n",
    "np.random.seed(Config.RANDOM_SEED)\n",
    "\n",
    "# Load image and mask paths\n",
    "print(\"\\nLoading dataset...\")\n",
    "image_paths = sorted(glob(os.path.join(Config.IMAGES_PATH, '*.jpg')))\n",
    "mask_paths = sorted(glob(os.path.join(Config.MASKS_PATH, '*.jpg')))\n",
    "\n",
    "print(f\"âœ“ Found {len(image_paths)} images\")\n",
    "print(f\"âœ“ Found {len(mask_paths)} masks\")\n",
    "\n",
    "# Verify dataset integrity\n",
    "assert len(image_paths) == len(mask_paths), \\\n",
    "    f\"Mismatch: {len(image_paths)} images vs {len(mask_paths)} masks\"\n",
    "assert len(image_paths) > 0, \"No images found! Check your dataset path.\"\n",
    "\n",
    "# Display sample filenames\n",
    "print(\"\\nSample files:\")\n",
    "for i in range(min(3, len(image_paths))):\n",
    "    print(f\"  Image {i+1}: {os.path.basename(image_paths[i])}\")\n",
    "    print(f\"  Mask  {i+1}: {os.path.basename(mask_paths[i])}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATASET SPLITTING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# First split: separate test set\n",
    "train_imgs, temp_imgs, train_masks, temp_masks = train_test_split(\n",
    "    image_paths, mask_paths, \n",
    "    test_size=(1 - Config.TRAIN_RATIO), \n",
    "    random_state=Config.RANDOM_SEED\n",
    ")\n",
    "\n",
    "# Second split: separate validation and test sets\n",
    "val_size = Config.TEST_RATIO / (Config.VAL_RATIO + Config.TEST_RATIO)\n",
    "val_imgs, test_imgs, val_masks, test_masks = train_test_split(\n",
    "    temp_imgs, temp_masks, \n",
    "    test_size=val_size,\n",
    "    random_state=Config.RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset split:\")\n",
    "print(f\"  Training:   {len(train_imgs):4d} samples ({len(train_imgs)/len(image_paths)*100:.1f}%)\")\n",
    "print(f\"  Validation: {len(val_imgs):4d} samples ({len(val_imgs)/len(image_paths)*100:.1f}%)\")\n",
    "print(f\"  Test:       {len(test_imgs):4d} samples ({len(test_imgs)/len(image_paths)*100:.1f}%)\")\n",
    "print(f\"  Total:      {len(image_paths):4d} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41416a6-c4d0-40c2-bb10-e72ae64726c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = PolypDataset(train_imgs, train_masks, get_train_transforms())\n",
    "val_dataset = PolypDataset(val_imgs, val_masks, get_val_transforms())\n",
    "test_dataset = PolypDataset(test_imgs, test_masks, get_val_transforms())\n",
    "\n",
    "print(f\"\\nâœ“ Datasets created\")\n",
    "print(f\"  Training dataset:   {len(train_dataset)} samples\")\n",
    "print(f\"  Validation dataset: {len(val_dataset)} samples\")\n",
    "print(f\"  Test dataset:       {len(test_dataset)} samples\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799cacac-63de-46fb-869f-8160dbf4b792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=Config.BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    num_workers=Config.NUM_WORKERS, \n",
    "    pin_memory=Config.PIN_MEMORY,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=Config.BATCH_SIZE, \n",
    "    shuffle=False,\n",
    "    num_workers=Config.NUM_WORKERS, \n",
    "    pin_memory=Config.PIN_MEMORY\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=Config.BATCH_SIZE, \n",
    "    shuffle=False,\n",
    "    num_workers=Config.NUM_WORKERS, \n",
    "    pin_memory=Config.PIN_MEMORY\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ DataLoaders created\")\n",
    "print(f\"  Training batches:   {len(train_loader)}\")\n",
    "print(f\"  Validation batches: {len(val_loader)}\")\n",
    "print(f\"  Test batches:       {len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5f910d-25be-4dac-b55a-80cd15df1afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "print(\"\\nBuilding DeepLabV3+ model...\")\n",
    "model = DeepLabV3Plus(num_classes=1).to(Config.DEVICE)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"âœ“ Model created successfully\")\n",
    "print(f\"  Device: {Config.DEVICE}\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Loss function\n",
    "criterion = CombinedLoss(alpha=0.5)\n",
    "print(f\"\\nâœ“ Loss function: Combined BCE + Dice (Î±=0.5)\")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=Config.BASE_LR)\n",
    "print(f\"âœ“ Optimizer: Adam (lr={Config.BASE_LR})\")\n",
    "\n",
    "# Learning rate scheduler\n",
    "max_iterations = len(train_loader) * Config.EPOCHS\n",
    "scheduler = PolynomialLR(optimizer, max_iterations, Config.LR_POWER)\n",
    "print(f\"âœ“ Scheduler: Polynomial LR (power={Config.LR_POWER})\")\n",
    "print(f\"  Total iterations: {max_iterations:,}\")\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [], 'train_iou': [], 'train_dice': [],\n",
    "    'val_loss': [], 'val_iou': [], 'val_dice': []\n",
    "}\n",
    "\n",
    "best_val_iou = 0.0\n",
    "best_epoch = 0\n",
    "\n",
    "print(f\"\\nStarting training for {Config.EPOCHS} epochs...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for epoch in range(Config.EPOCHS):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Epoch {epoch+1}/{Config.EPOCHS}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_iou, train_dice = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, scheduler, Config.DEVICE\n",
    "    )\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_iou, val_dice = validate(\n",
    "        model, val_loader, criterion, Config.DEVICE\n",
    "    )\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_iou'].append(train_iou)\n",
    "    history['train_dice'].append(train_dice)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_iou'].append(val_iou)\n",
    "    history['val_dice'].append(val_dice)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\nðŸ“Š Epoch {epoch+1} Summary:\")\n",
    "    print(f\"   Train â†’ Loss: {train_loss:.4f} | IoU: {train_iou:.4f} | Dice: {train_dice:.4f}\")\n",
    "    print(f\"   Val   â†’ Loss: {val_loss:.4f} | IoU: {val_iou:.4f} | Dice: {val_dice:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_iou > best_val_iou:\n",
    "        best_val_iou = val_iou\n",
    "        best_epoch = epoch + 1\n",
    "        \n",
    "        # Save model checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_iou': val_iou,\n",
    "            'val_dice': val_dice,\n",
    "            'history': history\n",
    "        }\n",
    "        torch.save(checkpoint, os.path.join(Config.OUTPUT_PATH, 'best_model.pth'))\n",
    "        print(f\"   âœ“ NEW BEST MODEL SAVED! (IoU: {best_val_iou:.4f})\")\n",
    "print(f\"Best validation IoU: {best_val_iou:.4f} (Epoch {best_epoch})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57a4305-f360-4f14-9115-e581a3903858",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(\n",
    "    history, \n",
    "    os.path.join(Config.OUTPUT_PATH, 'training_history.png')\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
